'''
Vineet Kumar, sioom.ai

This input file has a unique set of user-programmable hyper-parameters for
loading and testing a checkpointed file. It is envisioned that there will
be many input files, in the "input_param_files" directory, each with their
own unique set of hyperparameters.

Note the following:
	(1) This file should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "conversational-transaction-bot"
 
Command-line:
-------------
python3 ctbMain.py input_param_files/distilgpt2_params 

Path to ctb logs files:
-----------------------
It is the default directory, and the name of the file is "ctb_logs".

Path to Lightning logs files:
-----------------------------
It includes the following directories: (i) ctb_lightning_logs, (ii) name
of this file, (iii) a unique version number that increases every time this
file is used for training. Following is an example:
ctb_lightning_logs/distilgpt2_params/version_0
where "distilgpt2_params" is the name of this file

Path to Checkpointed files:
---------------------------
It includes the following directories: (i) path of Lightning logs files,
(ii) checkpoints. Following is an example:
ctb_lightning_logs/distilgpt2_params/version_0/checkpoints
where "distilgpt2_params" is the name of this file

Name of Checkpointed files:
---------------------------
During training, the checkpointed files are those that have the lowest
validation loss. The name includes the epoch number plus the value of the
validation loss. Following is an example:
epoch=00-val_loss=0.2329.ckpt
'''


# parameters for file "ctbMain.py"
{'chkpt': 'ctb_lightning_logs/distilgpt2_params/version_2/checkpoints/epoch=00-val_loss=0.2329.ckpt'}

# parameters for file "ctbModel.py"
{}

# parameters for file "ctbData.py"
{ 'default_format_path': 'data/dialog-bAbI-tasks/dstc2/defaultFormat.train', 'batch_size_test': 2} 

# parameters for Lightning Trainer
{'gpus': 1}

